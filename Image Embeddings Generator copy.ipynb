{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0fca28-c6e7-400f-bd76-f3e250504397",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8598733  0.78638345 0.7114111  0.8778672 ]\n"
     ]
    }
   ],
   "source": [
    "# Define existing text embeddings\n",
    "texts = [\n",
    "    \"A sleek and modern smartphone with 128GB storage and a powerful camera.\",\n",
    "    \"Wireless over-ear headphones with noise cancellation and 30-hour battery life.\",\n",
    "    \"Ergonomic office chair with lumbar support and adjustable height.\",\n",
    "    \"Gaming laptop with RTX 4060 GPU, 16GB RAM, and 1TB SSD storage.\"\n",
    "]\n",
    "\n",
    "# Tokenize and get embeddings for the reference texts\n",
    "inputs = processor(text=texts, return_tensors=\"pt\", padding=True)\n",
    "with torch.no_grad():\n",
    "    input_embeddings = model.get_text_features(**inputs)\n",
    "\n",
    "# **New input text to compare**\n",
    "new_text = [\"A modern and sleek laptop with 128GB storage and a camera.\"]\n",
    "\n",
    "# Tokenize and get embedding for the new text\n",
    "new_input = processor(text=new_text, return_tensors=\"pt\", padding=True)\n",
    "with torch.no_grad():\n",
    "    new_embedding = model.get_text_features(**new_input)\n",
    "\n",
    "# Compute cosine similarity\n",
    "similarities = cosine_similarity(new_embedding, input_embeddings)\n",
    "\n",
    "# Convert to numpy for better readability\n",
    "similarities = similarities.numpy()\n",
    "\n",
    "print(similarities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d90fb2d1-0a37-48e0-9dd0-07879a1f2a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "print(new_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1494ab-5260-458b-b8a8-51c83ca0a5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.8598733  0.78638345 0.7114111  0.8778672 ]\n",
    "torch.Size([1, 512])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9028da5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82e577c1-5017-4e62-a5c4-e40174d42d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "from torch.nn.functional import cosine_similarity\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Load CLIP model and processor\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15618546-d57b-4e6e-8817-23e069c6a074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4037253  0.41358453 0.24517404 0.6188889 ]\n",
      "[0.27472916 0.31950188 0.30867618 0.237894  ]\n"
     ]
    }
   ],
   "source": [
    "# **Reference text descriptions**\n",
    "texts = [\n",
    "    \"Clover Chibi with Jumbo darning Needle Set, 6.2'' Height x 2.1'' Length x 0.8'' Width, Multicolor\",\n",
    "    \"Clear Plastic Ornaments, Fillable for DIY Arts and Crafts (6.3 Inch, 6 Pack)\",\n",
    "    \"Design Works Crafts Friendship, 5 x 7 Counted Cross Stitch Kit White\",\n",
    "    \"DMC Stranded Cotton Number 3712\"\n",
    "]\n",
    "\n",
    "# Tokenize and get text embeddings\n",
    "inputs = processor(text=texts, return_tensors=\"pt\", padding=True)\n",
    "with torch.no_grad():\n",
    "    text_embeddings = model.get_text_features(**inputs)\n",
    "\n",
    "# **New input text to compare**\n",
    "new_text = [\"A modern and sleek laptop with 128GB storage and a camera.\"]\n",
    "new_input = processor(text=new_text, return_tensors=\"pt\", padding=True)\n",
    "with torch.no_grad():\n",
    "    new_text_embedding = model.get_text_features(**new_input)\n",
    "\n",
    "# **Load multiple images**\n",
    "image_urls = [\n",
    "    \"https://m.media-amazon.com/images/I/61cbPKvKXdL._AC_UL320_.jpg\",\n",
    "    \"https://m.media-amazon.com/images/I/61y3xtQVfYL._AC_UL320_.jpg\",\n",
    "    \"https://m.media-amazon.com/images/I/91BmFm22ZoL._AC_UL320_.jpg\",\n",
    "    \"https://m.media-amazon.com/images/I/51nJzzYy8ZL._AC_UL320_.jpg\"]\n",
    "\n",
    "# Process multiple images\n",
    "image_embeddings = []\n",
    "for url in image_urls:\n",
    "    image = Image.open(requests.get(url, stream=True).raw)  # Load image from URL\n",
    "    image_inputs = processor(images=image, return_tensors=\"pt\")  # Preprocess image\n",
    "    with torch.no_grad():\n",
    "        img_embedding = model.get_image_features(**image_inputs)  # Get image features\n",
    "    image_embeddings.append(img_embedding)\n",
    "\n",
    "# Stack all image embeddings into a tensor\n",
    "image_embeddings = torch.vstack(image_embeddings)  # Shape: (num_images, embedding_dim)\n",
    "\n",
    "# **Compute similarities**\n",
    "text_similarities = cosine_similarity(new_text_embedding, text_embeddings).numpy()\n",
    "image_similarities = cosine_similarity(image_embeddings, text_embeddings).numpy()  # Shape: (num_images, num_texts)\n",
    "\n",
    "# **Print results**\n",
    "print(text_similarities)\n",
    "\n",
    "print(image_similarities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226eb230-fd5c-4afd-a8e1-9e68a47d168e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfb316f4-1357-433b-b27e-051f85447485",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "from torch.nn.functional import cosine_similarity\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Load CLIP model and processor\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(\"cuda\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc389cbe-5f28-4c31-8c59-8f0970c54640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.65080106 0.66882294 0.7217649  0.70273376 0.68470734]\n"
     ]
    }
   ],
   "source": [
    "# **Reference images (the dataset to search in)**\n",
    "reference_image_urls = [\n",
    "    \"https://m.media-amazon.com/images/I/91IlO3j4kPL._AC_UL320_.jpg\",\n",
    "    \"https://m.media-amazon.com/images/I/71pjRDo52OL._AC_UL320_.jpg\",\n",
    "    \"https://m.media-amazon.com/images/I/71OueK6amcL._AC_UL320_.jpg\",\n",
    "    \"https://m.media-amazon.com/images/I/51UzaUUSLQL._AC_UL320_.jpg\",\n",
    "    \"https://m.media-amazon.com/images/I/81SMwAAGp6L._AC_UL320_.jpg\"\n",
    "]\n",
    "\n",
    "# **Process and encode reference images**\n",
    "reference_embeddings = []\n",
    "for url in reference_image_urls:\n",
    "    image = Image.open(requests.get(url, stream=True).raw)  # Load image from URL\n",
    "    image_inputs = processor(images=image, return_tensors=\"pt\").to(\"cuda\")  # Preprocess image\n",
    "    with torch.no_grad():\n",
    "        img_embedding = model.get_image_features(**image_inputs)  # Extract embedding\n",
    "    reference_embeddings.append(img_embedding)\n",
    "\n",
    "# Stack reference embeddings into a tensor\n",
    "reference_embeddings = torch.vstack(reference_embeddings)  # Shape: (num_images, embedding_dim)\n",
    "\n",
    "# **Query image (the one to search for)**\n",
    "query_image_url = \"https://m.media-amazon.com/images/I/919miJcpi1L.jpg\"  # Change this to your query image\n",
    "query_image = Image.open(requests.get(query_image_url, stream=True).raw)\n",
    "\n",
    "# Process and encode the query image\n",
    "query_inputs = processor(images=query_image, return_tensors=\"pt\").to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "    query_embedding = model.get_image_features(**query_inputs)  # Extract query image embedding\n",
    "\n",
    "# **Compute cosine similarity** between the query image and reference images\n",
    "image_similarities = cosine_similarity(query_embedding, reference_embeddings).cpu().numpy()\n",
    "\n",
    "# **Print results**\n",
    "print(image_similarities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47e79b5e-6401-4ab5-bb72-91faa4d1b155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU Memory: 16883.91 MB\n",
      "Allocated GPU Memory: 14397.07 MB\n",
      "Reserved GPU Memory: 15292.43 MB\n",
      "Free GPU Memory: 1591.48 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# FULL MODEL\n",
    "if torch.cuda.is_available():\n",
    "    gpu_info = torch.cuda.get_device_properties(0)\n",
    "    total_memory = gpu_info.total_memory / 1e6  # Convert to MB\n",
    "    allocated_memory = torch.cuda.memory_allocated() / 1e6  # Convert to MB\n",
    "    reserved_memory = torch.cuda.memory_reserved() / 1e6  # Convert to MB\n",
    "    free_memory = total_memory - reserved_memory  # Free memory estimation\n",
    "\n",
    "    print(f\"Total GPU Memory: {total_memory:.2f} MB\")\n",
    "    print(f\"Allocated GPU Memory: {allocated_memory:.2f} MB\")\n",
    "    print(f\"Reserved GPU Memory: {reserved_memory:.2f} MB\")\n",
    "    print(f\"Free GPU Memory: {free_memory:.2f} MB\")\n",
    "else:\n",
    "    print(\"No GPU detected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8a4bb14a-2a1b-4d00-9208-735a5aec0be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU Memory: 16883.91 MB\n",
      "Allocated GPU Memory: 8032.87 MB\n",
      "Reserved GPU Memory: 9193.91 MB\n",
      "Free GPU Memory: 7689.99 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 4 BIT QUANITZATION\n",
    "if torch.cuda.is_available():\n",
    "    gpu_info = torch.cuda.get_device_properties(0)\n",
    "    total_memory = gpu_info.total_memory / 1e6  # Convert to MB\n",
    "    allocated_memory = torch.cuda.memory_allocated() / 1e6  # Convert to MB\n",
    "    reserved_memory = torch.cuda.memory_reserved() / 1e6  # Convert to MB\n",
    "    free_memory = total_memory - reserved_memory  # Free memory estimation\n",
    "\n",
    "    print(f\"Total GPU Memory: {total_memory:.2f} MB\")\n",
    "    print(f\"Allocated GPU Memory: {allocated_memory:.2f} MB\")\n",
    "    print(f\"Reserved GPU Memory: {reserved_memory:.2f} MB\")\n",
    "    print(f\"Free GPU Memory: {free_memory:.2f} MB\")\n",
    "else:\n",
    "    print(\"No GPU detected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23627772-c9f9-4049-b914-710cba8e3b64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "custom-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
