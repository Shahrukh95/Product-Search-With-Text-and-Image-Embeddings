{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52375fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.6.0+cu124\n",
      "CUDA Available: True\n",
      "CUDA Version: 12.4\n",
      "Number of GPUs: 1\n",
      "Current GPU: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA Version:\", torch.version.cuda)\n",
    "    print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "    print(\"Current GPU:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "else:\n",
    "    print(\"CUDA is not available. Running on CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "424521bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\My Works\\Challenges\\Sereact\\Embeddings-pipeline\\custom-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\My Works\\Challenges\\Sereact\\Embeddings-pipeline\\custom-env\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ShahrukhAzharAhsan\\.cache\\huggingface\\hub\\models--openai--clip-vit-large-patch14. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "from torch.nn.functional import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "# Load CLIP model and processor\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a46fe15d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>title</th>\n",
       "      <th>imgUrl</th>\n",
       "      <th>productURL</th>\n",
       "      <th>stars</th>\n",
       "      <th>reviews</th>\n",
       "      <th>price</th>\n",
       "      <th>category_id</th>\n",
       "      <th>isBestSeller</th>\n",
       "      <th>boughtInLastMonth</th>\n",
       "      <th>id</th>\n",
       "      <th>category_name</th>\n",
       "      <th>llava_generated_image_caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B08ZDQX51W</td>\n",
       "      <td>Original Replacement Dell 130W Laptop Charger ...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/61sADwl+YW...</td>\n",
       "      <td>https://www.amazon.com/dp/B08ZDQX51W</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0</td>\n",
       "      <td>24.98</td>\n",
       "      <td>65</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>Laptop Accessories</td>\n",
       "      <td>A black power bank, which is a portable charge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B01BPCTXHC</td>\n",
       "      <td>Griffin Elevator Stand for Laptops - Lift Your...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/710N2S69Nv...</td>\n",
       "      <td>https://www.amazon.com/dp/B01BPCTXHC</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0</td>\n",
       "      <td>35.00</td>\n",
       "      <td>65</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>Laptop Accessories</td>\n",
       "      <td>A laptop computer sitting on a stand or a dock...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin                                              title  \\\n",
       "0  B08ZDQX51W  Original Replacement Dell 130W Laptop Charger ...   \n",
       "1  B01BPCTXHC  Griffin Elevator Stand for Laptops - Lift Your...   \n",
       "\n",
       "                                              imgUrl  \\\n",
       "0  https://m.media-amazon.com/images/I/61sADwl+YW...   \n",
       "1  https://m.media-amazon.com/images/I/710N2S69Nv...   \n",
       "\n",
       "                             productURL  stars  reviews  price  category_id  \\\n",
       "0  https://www.amazon.com/dp/B08ZDQX51W    4.5        0  24.98           65   \n",
       "1  https://www.amazon.com/dp/B01BPCTXHC    4.6        0  35.00           65   \n",
       "\n",
       "   isBestSeller  boughtInLastMonth  id       category_name  \\\n",
       "0         False                  0  65  Laptop Accessories   \n",
       "1         False                  0  65  Laptop Accessories   \n",
       "\n",
       "                       llava_generated_image_caption  \n",
       "0  A black power bank, which is a portable charge...  \n",
       "1  A laptop computer sitting on a stand or a dock...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_products = pd.read_csv(\"dataset/textual/complete/final_products.csv\")\n",
    "df_products.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a3c3d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Title of Product: Original Replacement Dell 130W Laptop Charger USB C Slim AC Power Adapter for Dell Xps 17,Precision 5550 5530 2in1,XPS 15 2in1 9575ï¼ŒDA130PM170 HA130PM170 0K00F5 K00F5 0M0H25 M0H25 T4V18\\nProduct Image Description: A black power bank, which is a portable charger used to charge electronic devices.\\nProduct Category: Laptop Accessories',\n",
       " 'Title of Product: Griffin Elevator Stand for Laptops - Lift Your Laptop to a Comfortable Viewing Height, Space Grey\\nProduct Image Description: A laptop computer sitting on a stand or a docking station.\\nProduct Category: Laptop Accessories']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_array = df_products.apply(lambda row: f\"Title of Product: {row['title']}\\nProduct Image Description: {row['llava_generated_image_caption']}\\nProduct Category: {row['category_name']}\", axis=1).tolist()\n",
    "result_array[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c0fca28-c6e7-400f-bd76-f3e250504397",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (213 > 77). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Sequence length must be less than max_position_embeddings (got `sequence length`: 213 and max_position_embeddings: 77",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m inputs \u001b[38;5;241m=\u001b[39m processor(text\u001b[38;5;241m=\u001b[39mresult_array, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 6\u001b[0m     input_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_text_features\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(input_embeddings\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32md:\\My Works\\Challenges\\Sereact\\Embeddings-pipeline\\custom-env\\Lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:1259\u001b[0m, in \u001b[0;36mCLIPModel.get_text_features\u001b[1;34m(self, input_ids, attention_mask, position_ids, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1254\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1255\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[0;32m   1256\u001b[0m )\n\u001b[0;32m   1257\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1259\u001b[0m text_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1260\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1263\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1264\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1266\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1268\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m text_outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1269\u001b[0m text_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_projection(pooled_output)\n",
      "File \u001b[1;32md:\\My Works\\Challenges\\Sereact\\Embeddings-pipeline\\custom-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\My Works\\Challenges\\Sereact\\Embeddings-pipeline\\custom-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\My Works\\Challenges\\Sereact\\Embeddings-pipeline\\custom-env\\Lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:947\u001b[0m, in \u001b[0;36mCLIPTextTransformer.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    944\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m    945\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, input_shape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m--> 947\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    949\u001b[0m \u001b[38;5;66;03m# CLIP's text model uses causal mask, prepare it here.\u001b[39;00m\n\u001b[0;32m    950\u001b[0m \u001b[38;5;66;03m# https://github.com/openai/CLIP/blob/cfcffb90e69f37bf2ff1e988237a0fbe41f33c04/clip/model.py#L324\u001b[39;00m\n\u001b[0;32m    951\u001b[0m causal_attention_mask \u001b[38;5;241m=\u001b[39m _create_4d_causal_attention_mask(\n\u001b[0;32m    952\u001b[0m     input_shape, hidden_states\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mhidden_states\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m    953\u001b[0m )\n",
      "File \u001b[1;32md:\\My Works\\Challenges\\Sereact\\Embeddings-pipeline\\custom-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\My Works\\Challenges\\Sereact\\Embeddings-pipeline\\custom-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\My Works\\Challenges\\Sereact\\Embeddings-pipeline\\custom-env\\Lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:283\u001b[0m, in \u001b[0;36mCLIPTextEmbeddings.forward\u001b[1;34m(self, input_ids, position_ids, inputs_embeds)\u001b[0m\n\u001b[0;32m    280\u001b[0m max_position_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m seq_length \u001b[38;5;241m>\u001b[39m max_position_embedding:\n\u001b[1;32m--> 283\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    284\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSequence length must be less than max_position_embeddings (got `sequence length`: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    285\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseq_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and max_position_embeddings: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_position_embedding\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    286\u001b[0m     )\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m position_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    289\u001b[0m     position_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_ids[:, :seq_length]\n",
      "\u001b[1;31mValueError\u001b[0m: Sequence length must be less than max_position_embeddings (got `sequence length`: 213 and max_position_embeddings: 77"
     ]
    }
   ],
   "source": [
    "# Define existing text embeddings\n",
    "\n",
    "# Tokenize and get embeddings for the reference texts\n",
    "inputs = processor(text=result_array, return_tensors=\"pt\", padding=True)\n",
    "with torch.no_grad():\n",
    "    input_embeddings = model.get_text_features(**inputs)\n",
    "\n",
    "print(input_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "79899197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.8680e-01, -3.7609e-02, -1.4987e-01,  1.5071e-01,  5.2516e-02,\n",
      "         -2.2427e-01,  8.4733e-02, -1.5496e+00, -1.5143e-01,  1.6270e-01,\n",
      "         -2.1243e-01, -1.5685e-01, -5.7812e-02,  9.1316e-02,  1.8141e-01,\n",
      "         -1.3125e-01,  4.8740e-01,  1.5257e-01, -4.0088e-02,  3.4411e-02,\n",
      "          5.7694e-01, -1.9801e-01,  1.3401e-01, -8.7694e-02, -1.4660e-01,\n",
      "         -4.4457e-01, -4.7334e-02,  1.5470e-01,  4.8919e-02,  1.8446e-01,\n",
      "         -7.2227e-02, -2.5874e-01, -1.0869e-01, -1.9223e-01,  4.3651e-02,\n",
      "         -6.0508e-02,  9.1682e-02,  7.5310e-02,  1.5855e-02,  1.6280e-01,\n",
      "         -3.5181e-02, -9.9254e-02, -1.3517e-01,  2.5040e-01,  1.1593e-01,\n",
      "          2.7636e-01,  7.6172e-02, -1.8008e-01,  1.2322e-01, -3.8321e-02,\n",
      "          4.2785e-02, -4.0468e-02, -1.4017e-01, -1.9123e-01, -1.0362e-01,\n",
      "         -5.3936e-02, -1.8461e-01, -6.9478e-02, -7.3263e-02, -1.0235e-01,\n",
      "          5.5536e-01, -8.4509e-03,  3.2389e-01, -1.4395e-01,  1.8065e-01,\n",
      "         -1.7176e-01, -4.4324e-02, -2.0071e-01, -2.2696e-01,  1.6115e-01,\n",
      "         -1.3543e-01, -1.3146e-01, -9.8603e-02, -1.1786e-01,  2.8101e-01,\n",
      "         -4.4979e-01,  7.3594e-02,  3.6780e-01, -1.7164e-01, -5.1307e-01,\n",
      "          1.4797e-01, -1.9367e-01, -2.8566e-01,  9.6584e-02, -1.0926e-01,\n",
      "         -3.0838e-02,  1.0196e-01, -2.4979e-01, -1.2654e-02,  1.3038e-01,\n",
      "          4.1241e-01,  1.3777e-01, -1.8788e+00,  1.9071e-01,  3.1120e-02,\n",
      "         -4.2539e-02, -1.0689e-01,  2.7608e-02, -7.5779e-02, -8.9279e-02,\n",
      "          1.1649e-01,  6.8109e-02,  2.0973e-01,  1.7080e-01, -4.5881e-02,\n",
      "          5.7790e-03,  4.7255e-01, -9.9797e-02, -1.2536e-01,  7.4392e-02,\n",
      "          9.4386e-02,  3.3956e-01, -1.7808e-02,  3.4767e-02,  3.2860e-03,\n",
      "         -4.2663e-01,  1.3763e-01,  7.9769e-02,  5.6902e-02, -9.8939e-02,\n",
      "          4.2847e-01, -6.8473e-02,  6.9286e-02,  1.6340e-01,  6.1407e-02,\n",
      "         -1.8177e-01, -7.4751e-02, -6.3779e-02, -8.3065e-02, -1.5604e-01,\n",
      "          9.1168e-02,  6.1913e-02,  3.6905e-01,  8.0187e+00, -1.0129e-01,\n",
      "          4.6923e-02,  1.2986e-01, -4.8802e-01, -1.5458e-01, -7.4699e-02,\n",
      "         -1.1239e-01,  1.4901e-02, -1.6676e-01,  1.7732e-01, -7.7416e-02,\n",
      "          2.7190e-01, -4.1513e-01,  6.8678e-02,  1.1133e-02,  2.6412e-02,\n",
      "          9.2699e-02, -1.7257e-01, -1.8264e-01,  9.1126e-02,  2.7340e-01,\n",
      "         -1.1029e-01, -5.5870e-02, -1.6577e-01, -4.5784e-01, -4.8352e-02,\n",
      "         -3.1836e-01,  9.6238e-02,  1.3583e-02,  2.0967e-02,  8.1609e-02,\n",
      "          8.2777e-02,  2.1586e-01, -9.4253e-02,  1.0792e-01, -5.8870e-02,\n",
      "         -6.5086e-02,  4.2148e-02, -3.6657e-02,  3.3858e-02, -2.8012e-01,\n",
      "          8.3264e-02, -2.7544e-01, -8.0589e-02,  6.9127e-02,  7.0969e-02,\n",
      "          2.1284e-01,  1.9187e-01,  2.4062e-02,  1.4484e-01,  2.1553e-02,\n",
      "          1.5194e-01,  2.0383e-01, -1.5391e-01,  1.5549e-01, -1.1662e-01,\n",
      "          2.7849e-01, -6.8070e-02,  1.5809e-01,  3.6696e-01,  1.6994e-01,\n",
      "          6.1493e-02, -4.8370e-02,  6.3303e-02, -4.5864e-01,  9.7036e-02,\n",
      "          1.1023e-01, -1.4340e-01,  6.7428e-02,  7.7497e-02,  2.5715e-01,\n",
      "         -6.8418e-02, -1.1055e-01,  6.0077e-02, -6.4704e-02,  1.7115e-01,\n",
      "          3.5514e-01, -1.0101e-01, -1.2544e-01,  1.5300e-01, -1.0068e-01,\n",
      "         -1.7756e-01, -8.0459e-02, -3.0551e-01,  4.4241e-01, -6.3878e-02,\n",
      "          1.0905e-01,  8.4673e-02, -2.0875e-01, -1.7823e-01,  4.7772e-02,\n",
      "         -7.7871e-02, -2.0621e-01,  3.1244e-01, -9.4091e-02,  2.1571e-03,\n",
      "         -4.1286e-01,  3.0191e-01,  1.2970e-01,  5.2129e-02,  7.7303e-02,\n",
      "          1.2930e-03,  1.9924e-01, -2.7471e-01,  3.4573e-02, -1.0807e-01,\n",
      "          1.1594e-01,  2.2706e-01,  1.6490e-01, -3.3616e-01, -7.8936e-02,\n",
      "          1.8109e-01,  1.6362e-01, -3.3955e-01,  1.8250e-01, -1.1680e-01,\n",
      "         -5.6240e-02, -4.7514e-02,  1.6120e-01, -1.9207e-01, -1.8529e-03,\n",
      "          5.6274e-02,  3.5916e-02,  6.5712e-02,  1.0708e-01, -1.7046e-01,\n",
      "         -1.1062e-01, -1.7951e-01, -1.2110e-01,  2.8167e-02, -2.5002e-01,\n",
      "          2.1820e-01, -3.1345e-02, -2.8785e-01,  8.4796e-02, -1.6182e-01,\n",
      "          2.5344e-02, -2.4292e-01,  1.8900e-01,  3.7621e-02, -9.0818e-02,\n",
      "         -1.1906e-02, -1.2842e-01,  2.8114e-01, -8.6088e-02,  1.0591e-01,\n",
      "          1.8351e-01,  2.4281e-01,  2.1364e-01, -1.8157e-01,  8.7102e-02,\n",
      "          2.1000e-01,  1.4749e-01, -4.5762e-02, -7.0820e-02, -2.4425e-01,\n",
      "          3.5945e-01,  2.8778e-02,  3.2063e-01,  2.1198e-01,  9.0124e-02,\n",
      "          1.1177e-01,  9.9764e-03, -3.3824e-01,  1.5925e-01, -5.2089e-02,\n",
      "          1.6520e-01, -1.5202e-01,  1.8803e-01,  8.1499e-02,  1.1372e-01,\n",
      "          1.4426e-01, -2.7440e-01, -1.5553e-01, -4.1329e-02,  1.5869e-02,\n",
      "          2.0380e-01, -1.1269e-02,  8.0087e+00, -2.0948e-02,  2.1197e-01,\n",
      "          6.2746e-03, -1.3189e-01, -6.1891e-02, -1.7167e-01,  3.7944e-02,\n",
      "          2.1634e-01,  5.3240e-01, -1.3457e-01,  2.6745e-01,  1.0495e-01,\n",
      "         -1.0399e-01,  5.9674e-02, -2.9302e-01,  5.5696e-02, -2.9472e+00,\n",
      "         -8.4012e-02, -1.9465e-01, -6.3320e-03,  7.2091e-02,  2.5403e-01,\n",
      "         -2.6188e-01, -2.3660e-03, -2.7905e-02,  4.6344e-02, -1.3098e-01,\n",
      "         -4.2584e-02,  1.2293e-01,  4.9616e-02,  1.4489e-01, -1.4526e-01,\n",
      "         -7.0423e-02, -1.5061e-02,  1.8007e-01, -5.3749e-02, -1.2900e-01,\n",
      "          1.9048e-01, -1.1866e-02, -9.4687e-03, -2.0431e-02, -2.3973e-01,\n",
      "          4.1439e-01,  2.2932e-02,  2.6826e-01, -2.1911e-02, -3.1761e-01,\n",
      "          3.9034e-02,  1.0521e-01,  4.5745e-01,  4.5590e-01,  2.5835e-01,\n",
      "         -1.7243e-01,  1.9016e-01, -1.2963e-01, -1.2178e-01,  3.4684e-01,\n",
      "         -1.3505e-01, -2.9150e-02, -1.5049e-01, -1.0099e-01, -7.0848e-02,\n",
      "          7.2551e-02, -6.5342e-02, -2.4531e-02, -2.8208e-01, -1.7438e-02,\n",
      "          9.6851e-02, -2.2448e-01,  8.7771e-02,  2.3165e-02,  3.1380e-01,\n",
      "          1.2109e-01, -1.1735e-01,  3.1873e-01, -1.0266e-01, -1.4694e-01,\n",
      "         -2.0826e-01, -2.9459e-01, -1.2824e-01, -3.3579e-02,  1.1480e-01,\n",
      "         -3.7621e-02, -6.2465e-02, -1.4266e-01, -2.9781e-01,  1.8959e-01,\n",
      "          2.5880e-01,  2.2390e-01, -1.1187e-02, -6.9476e-02, -6.7442e-02,\n",
      "         -2.9276e-02, -5.1544e-02,  6.5619e-02,  2.7706e-02, -9.1472e-02,\n",
      "          9.5331e-02,  2.0243e-01,  9.5054e-02,  8.7335e-03,  1.8897e-01,\n",
      "          1.0820e-01, -1.8660e-01, -4.5357e-03, -1.4364e-01, -6.8918e-02,\n",
      "          1.4446e-01, -5.4160e-02,  3.4299e-02, -1.7011e-01,  4.6088e-02,\n",
      "         -1.5659e-02, -2.0969e-01, -2.1463e-02,  4.5354e-02,  1.2970e-01,\n",
      "         -1.6979e-01,  4.9300e-02, -2.2095e-01, -6.2443e-02,  2.1143e-01,\n",
      "          1.6994e-01,  1.5965e-01, -3.7431e-01, -1.6226e-01, -1.2773e-02,\n",
      "          9.5893e-02,  4.2433e-02,  2.3665e-01, -1.2840e-01,  1.1479e-02,\n",
      "         -1.9687e-02, -2.8792e-02, -1.2713e-01,  8.3747e-02,  1.2745e-01,\n",
      "         -5.8891e-02,  4.2439e-02, -1.1371e-01,  2.3129e-01,  7.0634e-02,\n",
      "          1.9508e-01, -1.5530e-01,  2.4821e-01,  4.5109e-02, -4.7673e-03,\n",
      "          1.5627e-01, -1.4547e-01,  1.5791e-01,  2.3848e-01,  1.0126e-01,\n",
      "          3.5462e-02,  6.8358e-02, -1.4490e-01,  3.0468e-02,  6.4615e-02,\n",
      "         -2.6587e-02,  1.7348e-01, -1.5700e-01, -2.9822e-01,  2.2502e-01,\n",
      "          2.2255e-01,  4.4953e-02,  1.7074e-01,  1.6011e-01,  2.2037e-01,\n",
      "          1.9272e-01, -7.0802e-01, -6.4713e-02, -6.6369e-02, -3.4950e-01,\n",
      "         -2.6756e-02, -1.2216e-01,  1.5993e-01, -1.7240e-01,  2.5678e-02,\n",
      "          8.9731e-02,  1.3161e-01, -1.5140e-01,  7.0800e-01, -1.0702e-01,\n",
      "         -2.6109e-01, -2.0789e-01, -1.1607e-01,  5.3425e-02,  1.0878e-01,\n",
      "         -1.9678e-01, -1.3523e-01,  1.9231e-01,  1.2856e-01, -1.0805e-02,\n",
      "          2.5048e-01,  4.3659e-02, -3.8807e-02, -1.4081e-01, -5.6237e-01,\n",
      "          3.8293e-02,  2.4060e-01]])\n"
     ]
    }
   ],
   "source": [
    "print(input_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d90fb2d1-0a37-48e0-9dd0-07879a1f2a96",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_embedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mnew_embedding\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'new_embedding' is not defined"
     ]
    }
   ],
   "source": [
    "print(new_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1494ab-5260-458b-b8a8-51c83ca0a5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [0.8598733  0.78638345 0.7114111  0.8778672 ]\n",
    "# torch.Size([1, 512])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226eb230-fd5c-4afd-a8e1-9e68a47d168e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e79b5e-6401-4ab5-bb72-91faa4d1b155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU Memory: 16883.91 MB\n",
      "Allocated GPU Memory: 14397.07 MB\n",
      "Reserved GPU Memory: 15292.43 MB\n",
      "Free GPU Memory: 1591.48 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_info = torch.cuda.get_device_properties(0)\n",
    "    total_memory = gpu_info.total_memory / 1e6  # Convert to MB\n",
    "    allocated_memory = torch.cuda.memory_allocated() / 1e6  # Convert to MB\n",
    "    reserved_memory = torch.cuda.memory_reserved() / 1e6  # Convert to MB\n",
    "    free_memory = total_memory - reserved_memory  # Free memory estimation\n",
    "\n",
    "    print(f\"Total GPU Memory: {total_memory:.2f} MB\")\n",
    "    print(f\"Allocated GPU Memory: {allocated_memory:.2f} MB\")\n",
    "    print(f\"Reserved GPU Memory: {reserved_memory:.2f} MB\")\n",
    "    print(f\"Free GPU Memory: {free_memory:.2f} MB\")\n",
    "else:\n",
    "    print(\"No GPU detected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23627772-c9f9-4049-b914-710cba8e3b64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39314e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison\n",
    "# Define existing text embeddings\n",
    "texts = [\n",
    "    \"A sleek and modern smartphone with 128GB storage and a powerful camera.\",\n",
    "    \"Wireless over-ear headphones with noise cancellation and 30-hour battery life.\",\n",
    "    \"Ergonomic office chair with lumbar support and adjustable height.\",\n",
    "    \"Gaming laptop with RTX 4060 GPU, 16GB RAM, and 1TB SSD storage.\"\n",
    "]\n",
    "\n",
    "# Tokenize and get embeddings for the reference texts\n",
    "inputs = processor(text=texts, return_tensors=\"pt\", padding=True)\n",
    "with torch.no_grad():\n",
    "    input_embeddings = model.get_text_features(**inputs)\n",
    "\n",
    "# **New input text to compare**\n",
    "new_text = [\"A modern and sleek laptop with 128GB storage and a camera.\"]\n",
    "\n",
    "# Tokenize and get embedding for the new text\n",
    "new_input = processor(text=new_text, return_tensors=\"pt\", padding=True)\n",
    "with torch.no_grad():\n",
    "    new_embedding = model.get_text_features(**new_input)\n",
    "\n",
    "# Compute cosine similarity\n",
    "similarities = cosine_similarity(new_embedding, input_embeddings)\n",
    "\n",
    "# Convert to numpy for better readability\n",
    "similarities = similarities.numpy()\n",
    "\n",
    "print(similarities)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "custom-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
