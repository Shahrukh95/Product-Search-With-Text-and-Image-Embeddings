{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afaf7d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>title</th>\n",
       "      <th>imgUrl</th>\n",
       "      <th>productURL</th>\n",
       "      <th>stars</th>\n",
       "      <th>reviews</th>\n",
       "      <th>price</th>\n",
       "      <th>category_id</th>\n",
       "      <th>isBestSeller</th>\n",
       "      <th>boughtInLastMonth</th>\n",
       "      <th>id</th>\n",
       "      <th>category_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B08ZDQX51W</td>\n",
       "      <td>Original Replacement Dell 130W Laptop Charger ...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/61sADwl+YW...</td>\n",
       "      <td>https://www.amazon.com/dp/B08ZDQX51W</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0</td>\n",
       "      <td>24.98</td>\n",
       "      <td>65</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>Laptop Accessories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B01BPCTXHC</td>\n",
       "      <td>Griffin Elevator Stand for Laptops - Lift Your...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/710N2S69Nv...</td>\n",
       "      <td>https://www.amazon.com/dp/B01BPCTXHC</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0</td>\n",
       "      <td>35.00</td>\n",
       "      <td>65</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>Laptop Accessories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B0B2RKB1HP</td>\n",
       "      <td>Rolling Backpack, 17.3 inch Laptop Backpack wi...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/61EWjAcjla...</td>\n",
       "      <td>https://www.amazon.com/dp/B0B2RKB1HP</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0</td>\n",
       "      <td>89.99</td>\n",
       "      <td>65</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>Laptop Accessories</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin                                              title  \\\n",
       "0  B08ZDQX51W  Original Replacement Dell 130W Laptop Charger ...   \n",
       "1  B01BPCTXHC  Griffin Elevator Stand for Laptops - Lift Your...   \n",
       "2  B0B2RKB1HP  Rolling Backpack, 17.3 inch Laptop Backpack wi...   \n",
       "\n",
       "                                              imgUrl  \\\n",
       "0  https://m.media-amazon.com/images/I/61sADwl+YW...   \n",
       "1  https://m.media-amazon.com/images/I/710N2S69Nv...   \n",
       "2  https://m.media-amazon.com/images/I/61EWjAcjla...   \n",
       "\n",
       "                             productURL  stars  reviews  price  category_id  \\\n",
       "0  https://www.amazon.com/dp/B08ZDQX51W    4.5        0  24.98           65   \n",
       "1  https://www.amazon.com/dp/B01BPCTXHC    4.6        0  35.00           65   \n",
       "2  https://www.amazon.com/dp/B0B2RKB1HP    4.6        0  89.99           65   \n",
       "\n",
       "   isBestSeller  boughtInLastMonth  id       category_name  \n",
       "0         False                  0  65  Laptop Accessories  \n",
       "1         False                  0  65  Laptop Accessories  \n",
       "2         False                  0  65  Laptop Accessories  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_products = pd.read_csv(\"selected_products.csv\")\n",
    "df_products.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0929552d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration, BitsAndBytesConfig\n",
    "\n",
    "# Configure 4-bit quantization\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    # New way of increasing inference speed\n",
    "    # use_flash_attention_2=True\n",
    ")\n",
    "\n",
    "# Load the quantized model\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    \"llava-hf/llava-1.5-13b-hf\",\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-13b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fd064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import re\n",
    "\n",
    "def get_image_caption(image_url):\n",
    "    # Load a complete image\n",
    "    try:\n",
    "        image = Image.open(requests.get(image_url, stream=True).raw).convert(\"RGB\")\n",
    "    except:\n",
    "        return \"\"\n",
    "        \n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image},\n",
    "                {\"type\": \"text\", \"text\": \"What product is shown in this image? Describe the product only\"},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    #  Use 3rd person perspective to describe it.\n",
    "    # Describe the image as a product.\n",
    "    inputs = processor.apply_chat_template(\n",
    "        conversation,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device, torch.float16)\n",
    "\n",
    "    # Generate\n",
    "    generate_ids = model.generate(**inputs, max_new_tokens=300)\n",
    "    result = processor.batch_decode(generate_ids, skip_special_tokens=True)\n",
    "    return result\n",
    "\n",
    "def process_text(text):\n",
    "    # Extract text after \"ASSISTANT: The image shows \"\n",
    "    match = re.search(r\"ASSISTANT: The image shows (.*?)]\", text)\n",
    "    \n",
    "    if match:\n",
    "        extracted_text = match.group(1).strip()  # Get matched text\n",
    "        formatted_text = extracted_text[0].upper() + extracted_text[1:-1]  # Capitalize first letter\n",
    "        return formatted_text\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab712c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "for index, row in enumerate(df_products.itertuples()):\n",
    "    # if index < 288:\n",
    "    #     continue\n",
    "        \n",
    "    print(f\"Index No: {index}\\nID: {row.asin}\\n Image URL: {row.imgUrl}\")\n",
    "\n",
    "    caption = get_image_caption(row.imgUrl)\n",
    "    caption = process_text(str(caption))\n",
    "    \n",
    "    print(f\"Generated Caption: {caption}\\n\\n\")\n",
    "    \n",
    "    with open(\"captions.csv\", mode=\"a\", newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([row.asin, row.imgUrl, caption])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8a4198",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dc4b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d13175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A black power bank, which is a portable charger, sitting on a white background. The power bank is connected to a USB cable, which is plugged into a device. This product is designed to provide power to electronic devices when they are not connected to a traditional power source.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"['USER:  \\nWhat product is shown in this image? Describe the image as a product. ASSISTANT: The image shows a black power bank, which is a portable charger, sitting on a white background. The power bank is connected to a USB cable, which is plugged into a device. This product is designed to provide power to electronic devices when they are not connected to a traditional power source.']\"\n",
    "\n",
    "# Extract text after \"ASSISTANT: The image shows \"\n",
    "match = re.search(r\"ASSISTANT: The image shows (.*?)]\", text)\n",
    "\n",
    "if match:\n",
    "    extracted_text = match.group(1).strip()  # Get matched text\n",
    "    formatted_text = extracted_text[0].upper() + extracted_text[1:-1]  # Capitalize first letter\n",
    "    print(formatted_text)\n",
    "else:\n",
    "    print(\"ERROR: No match found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e056104",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1afcfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0fca28-c6e7-400f-bd76-f3e250504397",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8598733  0.78638345 0.7114111  0.8778672 ]\n"
     ]
    }
   ],
   "source": [
    "# Define existing text embeddings\n",
    "texts = [\n",
    "    \"A sleek and modern smartphone with 128GB storage and a powerful camera.\",\n",
    "    \"Wireless over-ear headphones with noise cancellation and 30-hour battery life.\",\n",
    "    \"Ergonomic office chair with lumbar support and adjustable height.\",\n",
    "    \"Gaming laptop with RTX 4060 GPU, 16GB RAM, and 1TB SSD storage.\"\n",
    "]\n",
    "\n",
    "# Tokenize and get embeddings for the reference texts\n",
    "inputs = processor(text=texts, return_tensors=\"pt\", padding=True)\n",
    "with torch.no_grad():\n",
    "    input_embeddings = model.get_text_features(**inputs)\n",
    "\n",
    "# **New input text to compare**\n",
    "new_text = [\"A modern and sleek laptop with 128GB storage and a camera.\"]\n",
    "\n",
    "# Tokenize and get embedding for the new text\n",
    "new_input = processor(text=new_text, return_tensors=\"pt\", padding=True)\n",
    "with torch.no_grad():\n",
    "    new_embedding = model.get_text_features(**new_input)\n",
    "\n",
    "# Compute cosine similarity\n",
    "similarities = cosine_similarity(new_embedding, input_embeddings)\n",
    "\n",
    "# Convert to numpy for better readability\n",
    "similarities = similarities.numpy()\n",
    "\n",
    "print(similarities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d90fb2d1-0a37-48e0-9dd0-07879a1f2a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "print(new_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1494ab-5260-458b-b8a8-51c83ca0a5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.8598733  0.78638345 0.7114111  0.8778672 ]\n",
    "torch.Size([1, 512])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9028da5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82e577c1-5017-4e62-a5c4-e40174d42d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "from torch.nn.functional import cosine_similarity\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Load CLIP model and processor\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15618546-d57b-4e6e-8817-23e069c6a074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4037253  0.41358453 0.24517404 0.6188889 ]\n",
      "[0.27472916 0.31950188 0.30867618 0.237894  ]\n"
     ]
    }
   ],
   "source": [
    "# **Reference text descriptions**\n",
    "texts = [\n",
    "    \"Clover Chibi with Jumbo darning Needle Set, 6.2'' Height x 2.1'' Length x 0.8'' Width, Multicolor\",\n",
    "    \"Clear Plastic Ornaments, Fillable for DIY Arts and Crafts (6.3 Inch, 6 Pack)\",\n",
    "    \"Design Works Crafts Friendship, 5 x 7 Counted Cross Stitch Kit White\",\n",
    "    \"DMC Stranded Cotton Number 3712\"\n",
    "]\n",
    "\n",
    "# Tokenize and get text embeddings\n",
    "inputs = processor(text=texts, return_tensors=\"pt\", padding=True)\n",
    "with torch.no_grad():\n",
    "    text_embeddings = model.get_text_features(**inputs)\n",
    "\n",
    "# **New input text to compare**\n",
    "new_text = [\"A modern and sleek laptop with 128GB storage and a camera.\"]\n",
    "new_input = processor(text=new_text, return_tensors=\"pt\", padding=True)\n",
    "with torch.no_grad():\n",
    "    new_text_embedding = model.get_text_features(**new_input)\n",
    "\n",
    "# **Load multiple images**\n",
    "image_urls = [\n",
    "    \"https://m.media-amazon.com/images/I/61cbPKvKXdL._AC_UL320_.jpg\",\n",
    "    \"https://m.media-amazon.com/images/I/61y3xtQVfYL._AC_UL320_.jpg\",\n",
    "    \"https://m.media-amazon.com/images/I/91BmFm22ZoL._AC_UL320_.jpg\",\n",
    "    \"https://m.media-amazon.com/images/I/51nJzzYy8ZL._AC_UL320_.jpg\"]\n",
    "\n",
    "# Process multiple images\n",
    "image_embeddings = []\n",
    "for url in image_urls:\n",
    "    image = Image.open(requests.get(url, stream=True).raw)  # Load image from URL\n",
    "    image_inputs = processor(images=image, return_tensors=\"pt\")  # Preprocess image\n",
    "    with torch.no_grad():\n",
    "        img_embedding = model.get_image_features(**image_inputs)  # Get image features\n",
    "    image_embeddings.append(img_embedding)\n",
    "\n",
    "# Stack all image embeddings into a tensor\n",
    "image_embeddings = torch.vstack(image_embeddings)  # Shape: (num_images, embedding_dim)\n",
    "\n",
    "# **Compute similarities**\n",
    "text_similarities = cosine_similarity(new_text_embedding, text_embeddings).numpy()\n",
    "image_similarities = cosine_similarity(image_embeddings, text_embeddings).numpy()  # Shape: (num_images, num_texts)\n",
    "\n",
    "# **Print results**\n",
    "print(text_similarities)\n",
    "\n",
    "print(image_similarities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226eb230-fd5c-4afd-a8e1-9e68a47d168e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfb316f4-1357-433b-b27e-051f85447485",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "from torch.nn.functional import cosine_similarity\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Load CLIP model and processor\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(\"cuda\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc389cbe-5f28-4c31-8c59-8f0970c54640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.65080106 0.66882294 0.7217649  0.70273376 0.68470734]\n"
     ]
    }
   ],
   "source": [
    "# **Reference images (the dataset to search in)**\n",
    "reference_image_urls = [\n",
    "    \"https://m.media-amazon.com/images/I/91IlO3j4kPL._AC_UL320_.jpg\",\n",
    "    \"https://m.media-amazon.com/images/I/71pjRDo52OL._AC_UL320_.jpg\",\n",
    "    \"https://m.media-amazon.com/images/I/71OueK6amcL._AC_UL320_.jpg\",\n",
    "    \"https://m.media-amazon.com/images/I/51UzaUUSLQL._AC_UL320_.jpg\",\n",
    "    \"https://m.media-amazon.com/images/I/81SMwAAGp6L._AC_UL320_.jpg\"\n",
    "]\n",
    "\n",
    "# **Process and encode reference images**\n",
    "reference_embeddings = []\n",
    "for url in reference_image_urls:\n",
    "    image = Image.open(requests.get(url, stream=True).raw)  # Load image from URL\n",
    "    image_inputs = processor(images=image, return_tensors=\"pt\").to(\"cuda\")  # Preprocess image\n",
    "    with torch.no_grad():\n",
    "        img_embedding = model.get_image_features(**image_inputs)  # Extract embedding\n",
    "    reference_embeddings.append(img_embedding)\n",
    "\n",
    "# Stack reference embeddings into a tensor\n",
    "reference_embeddings = torch.vstack(reference_embeddings)  # Shape: (num_images, embedding_dim)\n",
    "\n",
    "# **Query image (the one to search for)**\n",
    "query_image_url = \"https://m.media-amazon.com/images/I/919miJcpi1L.jpg\"  # Change this to your query image\n",
    "query_image = Image.open(requests.get(query_image_url, stream=True).raw)\n",
    "\n",
    "# Process and encode the query image\n",
    "query_inputs = processor(images=query_image, return_tensors=\"pt\").to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "    query_embedding = model.get_image_features(**query_inputs)  # Extract query image embedding\n",
    "\n",
    "# **Compute cosine similarity** between the query image and reference images\n",
    "image_similarities = cosine_similarity(query_embedding, reference_embeddings).cpu().numpy()\n",
    "\n",
    "# **Print results**\n",
    "print(image_similarities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47e79b5e-6401-4ab5-bb72-91faa4d1b155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU Memory: 16883.91 MB\n",
      "Allocated GPU Memory: 14397.07 MB\n",
      "Reserved GPU Memory: 15292.43 MB\n",
      "Free GPU Memory: 1591.48 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# FULL MODEL\n",
    "if torch.cuda.is_available():\n",
    "    gpu_info = torch.cuda.get_device_properties(0)\n",
    "    total_memory = gpu_info.total_memory / 1e6  # Convert to MB\n",
    "    allocated_memory = torch.cuda.memory_allocated() / 1e6  # Convert to MB\n",
    "    reserved_memory = torch.cuda.memory_reserved() / 1e6  # Convert to MB\n",
    "    free_memory = total_memory - reserved_memory  # Free memory estimation\n",
    "\n",
    "    print(f\"Total GPU Memory: {total_memory:.2f} MB\")\n",
    "    print(f\"Allocated GPU Memory: {allocated_memory:.2f} MB\")\n",
    "    print(f\"Reserved GPU Memory: {reserved_memory:.2f} MB\")\n",
    "    print(f\"Free GPU Memory: {free_memory:.2f} MB\")\n",
    "else:\n",
    "    print(\"No GPU detected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8a4bb14a-2a1b-4d00-9208-735a5aec0be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU Memory: 16883.91 MB\n",
      "Allocated GPU Memory: 8032.87 MB\n",
      "Reserved GPU Memory: 9193.91 MB\n",
      "Free GPU Memory: 7689.99 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 4 BIT QUANITZATION\n",
    "if torch.cuda.is_available():\n",
    "    gpu_info = torch.cuda.get_device_properties(0)\n",
    "    total_memory = gpu_info.total_memory / 1e6  # Convert to MB\n",
    "    allocated_memory = torch.cuda.memory_allocated() / 1e6  # Convert to MB\n",
    "    reserved_memory = torch.cuda.memory_reserved() / 1e6  # Convert to MB\n",
    "    free_memory = total_memory - reserved_memory  # Free memory estimation\n",
    "\n",
    "    print(f\"Total GPU Memory: {total_memory:.2f} MB\")\n",
    "    print(f\"Allocated GPU Memory: {allocated_memory:.2f} MB\")\n",
    "    print(f\"Reserved GPU Memory: {reserved_memory:.2f} MB\")\n",
    "    print(f\"Free GPU Memory: {free_memory:.2f} MB\")\n",
    "else:\n",
    "    print(\"No GPU detected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23627772-c9f9-4049-b914-710cba8e3b64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7942593a-6c84-428b-bfb7-388622665628",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "from PIL import Image\n",
    "\n",
    "# Load the model in half-precision\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-13b-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n",
    "processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-13b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1940c89-83bc-41dd-8ad1-d5f540fdbd26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration, BitsAndBytesConfig\n",
    "\n",
    "# Configure 4-bit quantization\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# Load the quantized model\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    \"llava-hf/llava-1.5-13b-hf\",\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-13b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84776fec-aeff-4321-9c2e-cb3ca1b60b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"/workspace/llava-1.5-13b-4bit\"  # Best location for large models\n",
    "model.save_pretrained(save_path)\n",
    "processor.save_pretrained(save_path)\n",
    "print(f\"Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b28f490-4deb-45f0-9dab-836ffa99aa83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    \"/workspace/llava-1.5-13b-4bit\",\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\"/workspace/llava-1.5-13b-4bit\")\n",
    "print(\"Quantized model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "21149797-91e0-4a46-8d00-615ae82286d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['USER:  \\nWhat product is shown in this image? ASSISTANT: The image shows a collection of various animal stickers, which are likely to be used for decoration or as a part of a scrapbooking project.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "from PIL import Image\n",
    "\n",
    "# Load a local image\n",
    "image_path = \"B0BB9CXN7J.jpg\"  # Replace with your local image path\n",
    "image = Image.open(image_path).convert(\"RGB\")  # Ensure RGB format\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\", \"text\": \"What product is shown in this image?\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "#  Use 3rd person perspective to describe it.\n",
    "inputs = processor.apply_chat_template(\n",
    "    conversation,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device, torch.float16)\n",
    "\n",
    "# Generate\n",
    "generate_ids = model.generate(**inputs, max_new_tokens=200)\n",
    "processor.batch_decode(generate_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d0a509-3a55-415e-90e4-355f1080224d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "custom-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
